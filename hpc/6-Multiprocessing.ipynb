{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `multiprocessing` library\n",
    "The multiprocessing module creates additional child processes with the same code as the parent process by either *forking* the parent on Unix systems (the OS snapshots the current running program into a new process). Each process runs its own Python interpreter and has its own GIL, each of which can utilize 100% of a CPU. Therefor if you have a quad-core processor and run four multiprocesses, it is possible to take advantage of 400% of your CPU.\n",
    "\n",
    "### Architecture\n",
    "<img src='figures/multiprocessing.png' width=500>\n",
    "\n",
    "It consists of a parent (main) program and multiple child processes (usually one per core). The parent program schedules work (provides input) for the children and consumes results (gathers output). Data is passed to and from children and the parent using the `pickle` module. When the parent process terminates, the child processes generally also terminate, though they can become orphaned and continue running on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Forking* causes multiple child processes to be instantiated, whereas *joining* causes child processes to be ended, and control is passed back to the primary process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tasks In Parallel\n",
    "Fit multiple models, cross validate them, and save them to disk. We will begin by writing three functions to generate a naive Bayes model, a logistic regression and a multilayer perceptron. Each function in turn creates three different models, defined by `Pipeline` that extract text from a corpus located at a specified path. Each task also determines a location to write the model to, and reports results using the logging module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextNormalizer, identity\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(path, saveto=None, cv=12):\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    if saveto is None:\n",
    "        saveto = 'naive_bayes_{}.pkl'.format(time.time())\n",
    "        \n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"naive bayes training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))\n",
    "    \n",
    "    \n",
    "def fit_logistic_regression(path, saveto=None, cv=12):\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    if saveto is None:\n",
    "        saveto = \"logistic_regression_{}.pkl\".format(time.time())\n",
    "    \n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"logistic regression training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))\n",
    "    \n",
    "def fit_multilayer_perceptron(path, saveto=None, cv=12):\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, lowercase=False)),\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=(10,10), early_stopping=True))\n",
    "    ])\n",
    "    if saveto is None:\n",
    "        saveto = \"multilayer_perceptron_{}.pkl\".format(time.time())\n",
    "    scores, delta = train_model(path, model, saveto, cv)\n",
    "    logger.info((\n",
    "        \"multilayer perceptron training took {:0.2f} seconds \"\n",
    "        \"with an average score of {:0.3f}\"\n",
    "    ).format(delta, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader import PickledCorpusReader\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents(corpus):\n",
    "    return [\n",
    "        list(corpus.docs(fileids=fileid))\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "def labels(corpus):\n",
    "    return [\n",
    "        corpus.categories(fileids=fileid)[0]\n",
    "        for fileid in corpus.fileids()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        return result, time.time() - start\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def train_model(path, model, saveto=None, cv=12):\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = PickledCorpusReader(path)\n",
    "    X = documents(corpus)\n",
    "    y = labels(corpus)\n",
    "    \n",
    "    # Compute cross validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv)\n",
    "    \n",
    "    # Fit the model on entire dataset\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(processName)-10s % (asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(path):\n",
    "    tasks = [\n",
    "        fit_naive_bayes, fit_logistic_regression, fit_multilayer_perceptron,\n",
    "    ]\n",
    "    logger.info('beginning parallel tasks')\n",
    "    start = time.time()\n",
    "    \n",
    "    procs = []\n",
    "    for task in tasks:\n",
    "        proc= mp.Process(name=task.__name__, target=task, args=(path,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        \n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "        \n",
    "    delta = time.time() - start\n",
    "    logger.info('total parallel fit time: {:0.2f} seconds'.format(delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
